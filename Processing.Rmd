---
title: "AQIProject"
author: "Jennifer Scott, Thammik Leungpathomaram"
date: "2026-02-13"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Library Block

```{r}
library(utils)
library(ggplot2)
library(dplyr)
library(caret)
library(corrplot)
library(tidyr)
library(leaps)
```


```{r}
source("config.R")
aqi <- read.csv(csv_path)
```


```{r}
#str(aqi)
summary(aqi)
```


```{r}
missing_summary <- aqi %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "n_missing") %>%
  mutate(pct_missing = n_missing / nrow(aqi) * 100) %>%
  arrange(desc(n_missing))
missing_summary




aqi_clean <- aqi %>% 
# Drop variables with > 50% missing data. We may need to add these back in. It is hurting the R2 of linear regression model.
  select(-PM25_ugm3, -PM10_ugm3) %>%
# Drop records that are missing the target variable, as imputation would be unreliable and could bias the model.
  filter(!is.na(Ozone_ppm))

aqi_clean$Date<-as.Date(aqi_clean$Date)

```

Variables with greater than 50% missingness or missing the target variable were removed to prevent unstable imputation and model distortion.

```{r}

# Applied median imputation for remaining numeric variables, as it's more robust to outliers than mean imputation.
aqi_clean <- aqi_clean %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), mean(., na.rm = TRUE), .)))
```



```{r}
ggplot(aqi_clean, aes(x = Ozone_ppm)) +
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
  labs(title = "Distribution of Ozone (ppm)", x = "Ozone (ppm)", y = "Count") +
  theme_minimal()

summary(aqi_clean$Ozone_ppm)
```

```{r}
ggplot(aqi_clean, aes(y = Ozone_ppm)) +
  geom_boxplot(fill = "steelblue", alpha = 0.7) +
  labs(title = "Ozone Distribution - Outlier Check", y = "Ozone (ppm)") +
  theme_minimal()
```


```{r}

ggplot(aqi_clean %>% filter(!is.na(Ozone_ppm)), 
       aes(x = Date, y = Ozone_ppm, color = as.factor(Year))) +
  geom_point(alpha = 0.6) +
  scale_x_date(date_breaks = "3 months", date_labels = "%b %Y") +
  labs(title = "Ozone Levels Over Time (2019-2021)",
       x = "Date", y = "Ozone (ppm)", color = "Ozone Level") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Same thing but dot plot instead. Trying to decide which one is better.

ggplot(aqi_clean, aes(x = Date, y = Ozone_ppm, color = as.factor(Year))) +
  geom_line(alpha = 0.6) +
  geom_smooth(aes(group = 1), method = "loess", formula = y ~ x, 
              color = "black", se = TRUE, alpha = 0.2) +
  scale_x_date(date_breaks = "3 months", date_labels = "%b %Y") +
  labs(title = "Ozone Levels Over Time (2019-2021)",
       x = "Date", y = "Ozone (ppm)", color = "Year") +
  theme_linedraw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}


ggplot(aqi_clean, aes(x = DayOfYear, y = Ozone_ppm)) +
  geom_point(alpha = 0.3, color = "steelblue") +
  geom_smooth(formula = y~x, method = "loess", color = "red", se = TRUE) +
  scale_x_continuous(breaks = c(1, 91, 182, 274, 365),
                     labels = c("Jan 1", "Apr 1", "Jul 1", "Oct 1", "Dec 31")) +
  labs(title = "Seasonal Pattern in Ozone Levels",
       x = "Day of Year", y = "Ozone (ppm)") +
  theme_minimal()

```

```{r}
ggplot(aqi_clean, aes(x = as.factor(Month), y = Ozone_ppm)) +
  geom_boxplot(fill = "steelblue", alpha = 0.7) +
  labs(title = "Ozone Distribution by Month",
       x = "Month", y = "Ozone (ppm)") +
  theme_minimal()
```





**Model Creation**

```{r}
#arbitrarily chose the temperature mean, and wind2m. DayOfYear for date purposes.
predictors <- aqi_clean %>%
  #select(where(is.numeric)) %>%
  select(-Ozone_ppm, -Ozone_ppb, -Ozone_HighDay, -TempMax_C, -TempMin_C, -Year,-Month,-DayOfWeek,-Date,-Wind10m_ms) %>%
  names()

#regression
lr_data <- aqi_clean %>%
  select(Ozone_ppm, all_of(predictors)) %>%
  drop_na()


```


Correlations
```{r}
cor_matrix <- cor(lr_data)

high_cor <- which(abs(cor_matrix) > 0.7, arr.ind = TRUE)
high_cor_pairs <- data.frame(
  var1 = rownames(cor_matrix)[high_cor[,1]],
  var2 = colnames(cor_matrix)[high_cor[,2]],
  correlation = cor_matrix[high_cor]
) %>%
  distinct() %>%
  filter(var1!=var2) %>%
  # Removes 'duplicate' pairs
  filter(!duplicated(paste(pmin(var1, var2), pmax(var1, var2)))) %>%
  arrange(desc(abs(correlation)))
high_cor_pairs

corrplot(cor_matrix)

```
Mostly variables I'd expect to be correlated with one another. 



<!-- ```{r}
set.seed(1)

#  train-validation-test follows a 50-30-20 split.
train_idx <- createDataPartition(lr_data$Ozone_ppm, p = 0.5, list = FALSE)
train_data <- lr_data[train_idx, ]
temp_data <- lr_data[-train_idx, ]

val_idx <- createDataPartition(temp_data$Ozone_ppm, p = 0.6, list = FALSE)
val_data <- temp_data[val_idx, ]
test_data <- temp_data[-val_idx, ]
``` -->
**Split Data**
```{r}
# Time based train / Test split

aqi_clean <- aqi_clean %>%
  mutate(Year = lubridate::year(Date))

train_data <- aqi_clean %>% filter(Year %in% c(2019, 2020))
test_data  <- aqi_clean %>% filter(Year == 2021)

dim(train_data)
dim(test_data)

```
**Regression Code. Currently uses all predictors.**
```{r}
formula_full <- as.formula(paste("Ozone_ppm ~", paste(predictors, collapse = " + ")))
lm_full <- lm(formula_full, data = train_data)


summary(lm_full)
```


```{r}
trControl <- trainControl(method = "none")

# Stepwise selection 
model_stepwise <- train(
  formula_full,
  data = train_data,
  trControl = trControl,
  method = "glmStepAIC",
  direction = "both",
  trace = FALSE
)
```

```{r}
lm_stepwise <- model_stepwise$finalModel

coef(lm_stepwise)
```
```{r}
summary(lm_stepwise)
lm_stepwise_r2<-1 - (lm_stepwise$deviance / lm_stepwise$null.deviance)
n <- nrow(train_data)
p <- length(coef(lm_stepwise)) - 1
lm_stepwise_adj_r2 <- 1 - (1 - lm_stepwise_r2) * (n - 1) / (n - p - 1)
lm_stepwise_r2
lm_stepwise_adj_r2
```

Only removes 1 predictor... Forward and backward have similar performance. Tried using best subset selection instead, but there's way too many predictors for that.

72% is decent, but 11 predictors is a lot.
```{r}

regfit_full <- regsubsets(formula_full, data = train_data, nvmax = 19)
reg_summary <- summary(regfit_full)
print(reg_summary)

subset_metrics <- data.frame(
  n_predictors = 1:length(reg_summary$rsq),
  rsq = reg_summary$rsq,
  adj_rsq = reg_summary$adjr2,
  cp = reg_summary$cp,
  bic = reg_summary$bic
)
print(subset_metrics)
```

Try removing predictors where p >0.05. (CO_ppm and Pressure_kPa)
```{r}
significant_vars <- c("NO2_ppb_or_ppm", "SO2_ppb_or_ppm", 
                      "Wind2m_ms", "DewPoint_C", "Precip_mm", "Solar_Wm2", "DayOfYear")
# removed these two for now "PM25_ugm3", "PM10_ugm3", but may need to add these back in. R2 is much worse without them 

formula_reduced <- as.formula(paste("Ozone_ppm ~", paste(significant_vars, collapse = " + ")))
lm_reduced <- lm(formula_reduced, data = train_data)

summary(lm_reduced)
```
Considering the minimal R^2 loss, I'd consider this to be a reasonable tradeoff for having 2 fewer predictors.
