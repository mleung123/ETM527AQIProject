---
title: "AQIProject"
author: "Jennifer Scott, Thammik Leungpathomaram"
date: "2026-02-13"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Library Block

```{r}
library(utils)
library(ggplot2)
library(dplyr)
library(caret)
library(corrplot)
library(tidyr)
library(leaps)
library(forecast)
library(zoo)
```


```{r}
source("config.R")
aqi <- read.csv(csv_path)
```


```{r}
#str(aqi)
#summary(aqi)
```


```{r}
missing_summary <- aqi %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "n_missing") %>%
  mutate(pct_missing = n_missing / nrow(aqi) * 100) %>%
  arrange(desc(n_missing))
missing_summary

aqi$Date<-as.Date(aqi$Date)

impute<-FALSE


aqi_clean <- aqi %>% 
# Drop variables with > 50% missing data. We may need to add these back in. It is hurting the R2 of linear regression model.
  select(-PM25_ugm3, -PM10_ugm3)

# Not having continuous data significantly complicates time series models.
# As such the below code will impute the missing days via linear interpolation.
# Also, ozone is an environmental variable, and as such changes slowly, which lessens the issues that this might cause.
if (impute){
  aqi$Ozone_ppm <- na.approx(aqi$Ozone_ppm, na.rm = FALSE)
} else {
  # Drop records that are missing the target variable, as imputation would be unreliable and could bias the model.
  aqi_clean<-filter(aqi,!is.na(Ozone_ppm))
}

# Quick check of the missing days. 29 days, consisting of stretches between 3-12 days.
#missing_days <-aqi[is.na(aqi$Ozone_ppm), ]$Date

```

Variables with greater than 50% missingness or missing the target variable were removed to prevent unstable imputation and model distortion.

```{r}

# Applied median imputation for remaining numeric variables, as it's more robust to outliers than mean imputation.
aqi_clean <- aqi_clean %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), mean(., na.rm = TRUE), .)))
```



```{r}
ggplot(aqi_clean, aes(x = Ozone_ppm)) +
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
  labs(title = "Distribution of Ozone (ppm)", x = "Ozone (ppm)", y = "Count") +
  theme_minimal()

summary(aqi_clean$Ozone_ppm)
```

```{r}
ggplot(aqi_clean, aes(y = Ozone_ppm)) +
  geom_boxplot(fill = "steelblue", alpha = 0.7) +
  labs(title = "Ozone Distribution - Outlier Check", y = "Ozone (ppm)") +
  theme_minimal()
```


```{r}

ggplot(aqi_clean %>% filter(!is.na(Ozone_ppm)), 
       aes(x = Date, y = Ozone_ppm, color = as.factor(Year))) +
  geom_point(alpha = 0.6) +
  scale_x_date(date_breaks = "3 months", date_labels = "%b %Y") +
  labs(title = "Ozone Levels Over Time (2019-2021)",
       x = "Date", y = "Ozone (ppm)", color = "Ozone Level") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Same thing but dot plot instead. Trying to decide which one is better.

ggplot(aqi_clean, aes(x = Date, y = Ozone_ppm, color = as.factor(Year))) +
  geom_line(alpha = 0.6) +
  geom_smooth(aes(group = 1), method = "loess", formula = y ~ x, 
              color = "black", se = TRUE, alpha = 0.2) +
  scale_x_date(date_breaks = "3 months", date_labels = "%b %Y") +
  labs(title = "Ozone Levels Over Time (2019-2021)",
       x = "Date", y = "Ozone (ppm)", color = "Year") +
  theme_linedraw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}


ggplot(aqi_clean, aes(x = DayOfYear, y = Ozone_ppm)) +
  geom_point(alpha = 0.3, color = "steelblue") +
  geom_smooth(formula = y~x, method = "loess", color = "red", se = TRUE) +
  scale_x_continuous(breaks = c(1, 91, 182, 274, 365),
                     labels = c("Jan 1", "Apr 1", "Jul 1", "Oct 1", "Dec 31")) +
  labs(title = "Seasonal Pattern in Ozone Levels",
       x = "Day of Year", y = "Ozone (ppm)") +
  theme_minimal()

```

```{r}
ggplot(aqi_clean, aes(x = as.factor(Month), y = Ozone_ppm)) +
  geom_boxplot(fill = "steelblue", alpha = 0.7) +
  labs(title = "Ozone Distribution by Month",
       x = "Month", y = "Ozone (ppm)") +
  theme_minimal()
```





**Model Creation**

```{r}
#arbitrarily chose the temperature mean, and wind2m. DayOfYear for date purposes.
predictors <- aqi_clean %>%
  #select(where(is.numeric)) %>%
  select(-Ozone_ppm, -Ozone_ppb, -Ozone_HighDay, -TempMax_C, -TempMin_C, -Year,-Month,-DayOfWeek,-Date,-Wind10m_ms) %>%
  names()

#regression
lr_data <- aqi_clean %>%
  select(Ozone_ppm, all_of(predictors)) %>%
  drop_na()


```


Correlations
```{r}
cor_matrix <- cor(lr_data)

high_cor <- which(abs(cor_matrix) > 0.7, arr.ind = TRUE)
high_cor_pairs <- data.frame(
  var1 = rownames(cor_matrix)[high_cor[,1]],
  var2 = colnames(cor_matrix)[high_cor[,2]],
  correlation = cor_matrix[high_cor]
) %>%
  distinct() %>%
  filter(var1!=var2) %>%
  # Removes 'duplicate' pairs
  filter(!duplicated(paste(pmin(var1, var2), pmax(var1, var2)))) %>%
  arrange(desc(abs(correlation)))
high_cor_pairs

corrplot(cor_matrix)

```
Mostly variables I'd expect to be correlated with one another. 



<!-- ```{r}
set.seed(1)

#  train-validation-test follows a 50-30-20 split.
train_idx <- createDataPartition(lr_data$Ozone_ppm, p = 0.5, list = FALSE)
train_data <- lr_data[train_idx, ]
temp_data <- lr_data[-train_idx, ]

val_idx <- createDataPartition(temp_data$Ozone_ppm, p = 0.6, list = FALSE)
val_data <- temp_data[val_idx, ]
test_data <- temp_data[-val_idx, ]
``` -->
**Split Data**
```{r}
# Time based train / Test split

aqi_clean <- aqi_clean %>%
  mutate(Year = lubridate::year(Date))

train_data <- aqi_clean %>% filter(Year %in% c(2019, 2020))
test_data  <- aqi_clean %>% filter(Year == 2021)

dim(train_data)
dim(test_data)

```
**Regression Code**

```{r}
formula_full <- as.formula(paste("Ozone_ppm ~", paste(predictors, collapse = " + ")))
lm_full <- lm(formula_full, data = train_data)
summary(lm_full)

cat("Train Adj R^2:", summary(lm_full)$adj.r.squared, "\n")

pred <- predict(lm_full, test_data)
actual <- test_data$Ozone_ppm
ss_res <- sum((actual - pred)^2)
ss_tot <- sum((actual - mean(actual))^2)
test_r2 <- 1 - ss_res / ss_tot
n_test <- nrow(test_data)
p <- length(coef(lm_full)) - 1
test_adj_r2 <- 1 - (1 - test_r2) * (n_test - 1) / (n_test - p - 1)
cat("Test Adj R^2: ", test_adj_r2, "\n")
```
Without Imputation:
Train Adj R^2: 0.6276748 
Test Adj R^2:  0.281179 

With Imputation:

Train Adj R^2: 0.593237 
Test Adj R^2:  0.2684621 

```{r}
trControl <- trainControl(method = "none")

# Stepwise selection 
model_stepwise <- train(
  formula_full,
  data = train_data,
  trControl = trControl,
  method = "glmStepAIC",
  direction = "both",
  trace = FALSE
)
```

```{r}
lm_stepwise <- model_stepwise$finalModel
```


```{r}

lm_stepwise_r2 <- 1 - (lm_stepwise$deviance / lm_stepwise$null.deviance)
n <- nrow(train_data)
p <- length(coef(lm_stepwise)) - 1
lm_stepwise_adj_r2 <- 1 - (1 - lm_stepwise_r2) * (n - 1) / (n - p - 1)
summary(lm_stepwise)
cat("Train Adj R^2:", lm_stepwise_adj_r2, "\n")

pred <- predict(lm_stepwise, test_data)
actual <- test_data$Ozone_ppm
ss_res <- sum((actual - pred)^2)
ss_tot <- sum((actual - mean(actual))^2)
test_r2 <- 1 - ss_res / ss_tot
n_test <- nrow(test_data)
test_adj_r2 <- 1 - (1 - test_r2) * (n_test - 1) / (n_test - p - 1)
cat("Test Adj R^2: ", test_adj_r2, "\n")


```
With Imputation:
Train Adj R^2: 0.5937668 
Test Adj R^2:  0.2704882 


Without Imputation:
Train Adj R^2: 0.6280177 
Test Adj R^2:  0.281659 
Only removes 1 predictor... Forward and backward have similar performance. Tried using best subset selection instead, but there's way too many predictors for that.

62% is decent, but 11 predictors is a lot.


Try removing predictors where p >0.05. (CO_ppm and Pressure_kPa)
```{r}
# removed these two for now "PM25_ugm3", "PM10_ugm3", but may need to add these back in. R2 is much worse without them 

significant_vars <- c("NO2_ppb_or_ppm", "SO2_ppb_or_ppm", 
                      "Wind2m_ms", "DewPoint_C", "Precip_mm", "Solar_Wm2", "DayOfYear")

formula_reduced <- as.formula(paste("Ozone_ppm ~", paste(significant_vars, collapse = " + ")))
lm_reduced <- lm(formula_reduced, data = train_data)
summary(lm_reduced)

cat("Train Adj R^2:", summary(lm_reduced)$adj.r.squared, "\n")

pred <- predict(lm_reduced, test_data)
actual <- test_data$Ozone_ppm
ss_res <- sum((actual - pred)^2)
ss_tot <- sum((actual - mean(actual))^2)
test_r2 <- 1 - ss_res / ss_tot
n_test <- nrow(test_data)
p <- length(coef(lm_reduced)) - 1
test_adj_r2 <- 1 - (1 - test_r2) * (n_test - 1) / (n_test - p - 1)
cat("Test Adj R^2: ", test_adj_r2, "\n")
```

With Imputation:
Train Adj R^2: 0.5363423 
Test Adj R^2:  0.3739211 

Without:

Train Adj R^2: 0.5819515 
Test Adj R^2:  0.2246784 

Acf Plot

```{r}

ozone.ts <- ts(train_data$Ozone_ppm, frequency = 365.25,
               start = c(as.numeric(format(min(train_data$Date), "%Y")),
                         as.numeric(format(min(train_data$Date), "%j"))))

# ACF plot
Acf(ozone.ts)

```

Very strong seasonality, with high ACF levels most of the way up to lag 200, and clear sinusoidal pattern.


# Since having dummy variables for each day would be impractical, I decided to use Fourier terms instead. We use the below plot to find the optimal K value. 
```{r}
k_values <- 1:10
aic_values <- sapply(k_values, function(k) {
  formula <- as.formula(paste("ozone.ts ~ trend + fourier(ozone.ts, K=", k, ") +", 
                               paste(significant_vars, collapse = " + ")))
  fit <- tslm(formula, data = train_data)
  return(AIC(fit))
})

ggplot(data.frame(K = k_values, AIC = aic_values), aes(x = K, y = AIC)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = k_values) +
  labs(title = "AIC vs Number of Fourier Terms (K)", x = "K", y = "AIC") +
  theme_minimal()
```
Based on the plot K=3 or 5 is ideal.


```{r}
# tslm without predictors
# very weak results (adjr^2=0.286)

#ozone.tslm <- tslm(ozone.ts ~ trend + fourier(ozone.ts, K=3))
#summary(ozone.tslm)
```

```{r}


# fourier tslm with predictors. 
formula1 <- as.formula(paste("ozone.ts ~ trend + fourier(ozone.ts, K=3) +", 
                             paste(predictors, collapse = " + ")))

formula2 <- as.formula(paste("ozone.ts ~ trend + fourier(ozone.ts, K=3) +", 
                             paste(significant_vars, collapse = " + ")))

ozone.tslm.fourier1 <- tslm(formula1, data = train_data)
ozone.tslm.fourier2 <- tslm(formula2, data = train_data)

#summary(ozone.tslm.fourier1)
summary(ozone.tslm.fourier1)$adj.r.squared
summary(ozone.tslm.fourier2)$adj.r.squared
```
With imputation: Adj R^2 of 66.6

Without imputation: Adjusted R^2 of 70.45% with K=5. Strong results, although this model has 22 predictors.

```{r}
test.ts <- ts(test_data$Ozone_ppm, frequency = 365,
              start = c(2021, 1))
#Doesn't work. TODO: troubleshoot this more.
#pred <- forecast(ozone.tslm.fourier1, newdata = test_data)
pred <- predict(ozone.tslm.fourier1, newdata = test_data)

accuracy(pred, test_data$Ozone_ppm)
```



Normal tlsm
```{r}
train_monthly <- train_data %>%
  group_by(Year, Month) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

test_monthly <- test_data %>%
  group_by(Year, Month) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

ozone.ts.monthly <- ts(train_monthly$Ozone_ppm, frequency = 12, start = c(2019, 1))

#formula1 <- as.formula(paste("ozone.ts.monthly ~ trend + season +", paste(predictors, collapse = " + ")))
ts_vars <- c("NO2_ppb_or_ppm", "SO2_ppb_or_ppm", "Wind2m_ms", 
                      "DewPoint_C", "Precip_mm", "Solar_Wm2")

formula2 <- as.formula(paste("ozone.ts.monthly ~ trend + season +", paste(ts_vars, collapse = " + ")))

#ozone.tslm.monthly1 <- tslm(formula1, data = train_monthly)
ozone.tslm.monthly2 <- tslm(formula2, data = train_monthly)

#cat("Train Adj R^2 (formula1):", summary(ozone.tslm.monthly1)$adj.r.squared, "\n")
cat("Train Adj R^2 (formula2):", summary(ozone.tslm.monthly2)$adj.r.squared, "\n")

```




```{r}
test.ts.monthly <- ts(test_monthly$Ozone_ppm, frequency = 12, start = c(2021, 1))

#forecast1 <- forecast(ozone.tslm.monthly1, newdata = test_monthly)
forecast2 <- forecast(ozone.tslm.monthly2, newdata = test_monthly[, significant_vars])

#cat("Train Adj R^2 (formula1):", summary(ozone.tslm.monthly1)$adj.r.squared, "\n")
cat("Train Adj R^2 (formula2):", summary(ozone.tslm.monthly2)$adj.r.squared, "\n")

#accuracy(forecast1, test.ts.monthly)
accuracy(forecast2, test.ts.monthly)
```




Predictor-less ARIMA
```{r}
ozone.arima <- auto.arima(ozone.ts)
summary(ozone.arima)

```

ARIMA with Predictors
```{r}
f
xreg_matrix <- as.matrix(train_data[, c("NO2_ppb_or_ppm", "CO_ppm", "SO2_ppb_or_ppm", 
                                         "Temp_C", "Wind2m_ms", "RH_percent", 
                                         "DewPoint_C", "Precip_mm", "Pressure_kPa", 
                                         "Solar_Wm2", "DayOfYear")])

ozone.arima.xreg <- auto.arima(ozone.ts, xreg = xreg_matrix)
summary(ozone.arima.xreg)
```
using predictors somehow makes ARIMA worse. 
